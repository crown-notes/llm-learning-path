# Introduction to LLMs

Language model are a computational tool that learns patterns and structures in language from large amounts of text data to predict and generate text. The key components of language model training are pre-training (unsupervised learning on vast datasets to learn general language) and fine-tuning (supervised learning on smaller, task-specific datasets to specialize the model).

There are various techniques used in language modelling like `N-Grams` and `Hidden Markov Models (HMMs)` to `transformer-based architectures.`

Language models have various usecases depending on the industry they are used in.

- Enhancing communication based on context.
- Critically evaluating generated information.
- Creating new careers.
- Automating time-consuming tasks

The difference between a `language model` and a `large-language model` is the scale. LLMs are much larger and more powerful, with billions or trillions of parameters. This size allows them to capture more intricate language details, understand context better, generate higher-quality text, and perform more complex tasks.
